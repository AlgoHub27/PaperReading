
伯努利随机变量的熵是信息论中的一个基本概念，它用来度量一个二元随机事件的不确定性。如果一个伯努利随机事件成功的概率为 $p$，那么失败的概率就是 $1−p$。伯努利随机变量的熵（通常用 $H_b(p)$ 表示）定义如下：

![](20250523234129.png|700x66)

这里的 $log⁡_2$ 表示以2为底的对数，用于计算比特（bits）作为信息量的单位。这个公式可以解释为：

- 当事件发生（成功）的概率是 $p$ 时，它提供的信息量是   $−log⁡_2(p)$   比特。
- 同样地，当事件不发生（失败）的概率是  ${1−p}$  时，相应的信息量是  $−log⁡_2(1-p)$  比特。

熵 $H_b(p)$ 是这两种情况的信息量的期望值，也就是它们各自发生的概率乘以其提供的信息量之和。因此，熵可以视为衡量对于一次伯努利试验结果的平均不确定度。熵的最大值发生在  $p=0.5$ 时，这时每次试验的结果最不可预测，熵值为1比特。随着  $p$ 值接近0或1，即事件越来越确定（几乎总是发生或几乎从不发生），熵会逐渐减小至0，因为此时结果变得非常可预测，不确定性也随之减少。

在论文中提到的伯努利源的例子中，使用了熵来描述一种特定情况下压缩二进制数据所需的最小比特率，这也是率失真理论的一部分，用于评估有损压缩算法的性能。在这种情况下，熵提供了一种方法来量化给定概率分布下的信息量，从而帮助确定编码效率。